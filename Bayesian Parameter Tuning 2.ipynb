{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.size'] = 18\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_folds = 5\n",
    "MAX_EVALS= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape:  (70000, 127)\n",
      "Testing features shape:  (30000, 127)\n"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('C:/Users/Richard Cheung/Desktop/Personal Project/training_data2.csv')\n",
    "\n",
    "features = features.sample(n = 100000, random_state = 42)\n",
    "\n",
    "features = features.select_dtypes('number')\n",
    "\n",
    "labels = np.array(features['target'].astype(np.int32)).reshape((-1, ))\n",
    "features = features.drop(columns = ['target'])\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features,\n",
    "                                                                            labels,\n",
    "                                                                            test_size = 30000,\n",
    "                                                                            random_state = 50)\n",
    "\n",
    "print('Training features shape: ', train_features.shape)\n",
    "print('Testing features shape: ', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(random_state = 50)\n",
    "\n",
    "train_set = lgb.Dataset(data = train_features, label = train_labels)\n",
    "test_set = lgb.Dataset(data = test_features, label = test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum validation RMSE was: 2601.02069 with a standard deviation of 204.35216.\n",
      "The optimal number of boosting rounds (estimators) was 519.\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = model.get_params()\n",
    "\n",
    "del hyperparameters['n_estimators']\n",
    "\n",
    "cv_results = lgb.cv(hyperparameters,\n",
    "                    train_set, \n",
    "                    num_boost_round = 10000,\n",
    "                    early_stopping_rounds = 100,\n",
    "                    metrics = 'rmse',\n",
    "                    verbose_eval = False,\n",
    "                    nfold = n_folds,\n",
    "                    seed = 42)\n",
    "\n",
    "best = cv_results['rmse-mean'][-1]\n",
    "\n",
    "best_std = cv_results['rmse-stdv'][-1]\n",
    "\n",
    "print('The maximum validation RMSE was: {:.5f} with a standard deviation of {:.5f}.'.format(best, best_std))\n",
    "print('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['rmse-mean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline model scores 2599.90545 RMSE on the test set.\n"
     ]
    }
   ],
   "source": [
    "model.n_estimators = len(cv_results['rmse-mean'])\n",
    "\n",
    "model.fit(train_features, train_labels)\n",
    "preds = model.predict(test_features)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(test_labels, preds))\n",
    "\n",
    "print('The baseline model scores {:.5f} RMSE on the test set.'.format(baseline_rmse))\n",
    "\n",
    "# This is the baseline score before hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from hyperopt import STATUS_OK\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def objective(hyperparameters):\n",
    "    \n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "    \n",
    "    if 'n_estimators' in hyperparameters:\n",
    "        del hyperparameters['n_estimators']\n",
    "        \n",
    "    subsample = hyperparameters['boosting_type'].get('subsample', 1.0)\n",
    "    \n",
    "    hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']\n",
    "    hyperparameters['subsample'] = subsample\n",
    "    \n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n",
    "        \n",
    "    start = timer()\n",
    "    \n",
    "    cv_results = lgb.cv(hyperparameters, \n",
    "                        train_set, \n",
    "                        num_boost_round = 10000,\n",
    "                        nfold = n_folds,\n",
    "                        early_stopping_rounds = 100,\n",
    "                        metrics = 'rmse',\n",
    "                        seed = 50)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "    best_score = cv_results['rmse-mean'][-1]\n",
    "    \n",
    "    n_estimators = len(cv_results['rmse-mean'])\n",
    "    \n",
    "    hyperparameters['n_estimators'] = n_estimators\n",
    "    \n",
    "    of_connection = open(OUT_FILE, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([best_score, hyperparameters, ITERATION, run_time])\n",
    "    of_connection.close()\n",
    "    \n",
    "    return {'loss': best_score, \n",
    "            'hyperparameters': hyperparameters, \n",
    "            'iteration': ITERATION,\n",
    "            'train_time': run_time,\n",
    "            'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': {'boosting_type': 'goss', 'subsample': 1.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosting_type = {'boosting_type': hp.choice('boosting_type',\n",
    "                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                                             {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                                             {'boosting_type': 'goss', 'subsample': 1.0}])}\n",
    "\n",
    "hyperparams = sample(boosting_type)\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {'boosting_type': hp.choice('boosting_type',\n",
    "                                    [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)},\n",
    "                                     {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                     {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
    "         'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n",
    "         'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n",
    "         'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "         'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "         'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "         'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "         'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
    "         'is_unbalance': hp.choice('is_unbalance', [True, False])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross validation score = 2582.59087.\n",
      "The optimal number of estimators was 2253.\n",
      "Wall time: 9min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "OUT_FILE = 'bayes_test2.csv'\n",
    "of_connection = open(OUT_FILE, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "headers = ['loss', 'hyperparameters', 'iteration', 'runtime']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()\n",
    "\n",
    "results = objective(sample(space))\n",
    "print('The cross validation score = {:.5f}.'.format(results['loss']))\n",
    "print('The optimal number of estimators was {}.'.format(results['hyperparameters']['n_estimators']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "\n",
    "tpe_algorithm = tpe.suggest\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FILE = 'bayes_test2.csv'\n",
    "of_connection = open(OUT_FILE, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "headers = ['loss', 'hyperparameters', 'iteration', 'runtime']\n",
    "writer.writerow(headers)\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5h 39min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from hyperopt import fmin\n",
    "\n",
    "global ITERATION\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            trials = trials,\n",
    "            max_evals = MAX_EVALS)\n",
    "\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 2577.860143720671,\n",
       "  'hyperparameters': {'boosting_type': 'gbdt',\n",
       "   'colsample_bytree': 0.6780349373400474,\n",
       "   'is_unbalance': True,\n",
       "   'learning_rate': 0.010388490944056066,\n",
       "   'min_child_samples': 55,\n",
       "   'num_leaves': 114,\n",
       "   'reg_alpha': 0.05611822193404954,\n",
       "   'reg_lambda': 0.9931142101637381,\n",
       "   'subsample_for_bin': 200000,\n",
       "   'subsample': 0.5006771377323579,\n",
       "   'metric': 'rmse',\n",
       "   'verbose': 1,\n",
       "   'n_estimators': 2422},\n",
       "  'iteration': 73,\n",
       "  'train_time': 484.2325275866169,\n",
       "  'status': 'ok'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials_dict = sorted(trials.results, key = lambda x: x['loss'])\n",
    "trials_dict[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(OUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def evaluate(results, name):\n",
    "    \n",
    "    new_results = results.copy()\n",
    "    \n",
    "    new_results['hyperparameters'] = new_results['hyperparameters'].map(ast.literal_eval)\n",
    "    \n",
    "    new_results = new_results.sort_values('loss', ascending = True).reset_index(drop = True)\n",
    "    \n",
    "    print('The lowest RMSE value from {} was {:.5f} found on iteration {}.'.format(name, new_results.loc[0, 'loss'], new_results.loc[0, 'iteration']))\n",
    "    \n",
    "    hyperparameters = new_results.loc[0, 'hyperparameters']\n",
    "    model = lgb.LGBMRegressor(**hyperparameters)\n",
    "    \n",
    "    model.fit(train_features, train_labels)\n",
    "\n",
    "    preds = model.predict(test_features)\n",
    "    \n",
    "    print('RMSE score from {} on test data = {:.5f}.'.format(name, np.sqrt(mean_squared_error(test_labels, preds))))\n",
    "    \n",
    "    hyp_df = pd.DataFrame(columns = list(new_results.loc[0, 'hyperparameters'].keys()))\n",
    "    \n",
    "    for i, hyp in enumerate(new_results['hyperparameters']):\n",
    "        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]),\n",
    "                               ignore_index = True)\n",
    "        \n",
    "    hyp_df['iteration'] = new_results['iteration']\n",
    "    hyp_df['score'] = new_results['loss']\n",
    "    \n",
    "    return hyp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest RMSE value from Bayesian was 2577.86014 found on iteration 73.\n",
      "RMSE score from Bayesian on test data = 2573.11412.\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bayes_results = evaluate(results, name = 'Bayesian')\n",
    "bayes_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
